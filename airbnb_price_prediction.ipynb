{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Data Preparation\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "\n",
    "This notebook contains the common data loading and preparation steps:\n",
    "- load data from the input CSV\n",
    "- do an assessment of the dataset to understand the number of distinct, missing, or invalid values by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0efbfb4ec663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "# common imports\n",
    "import zipfile\n",
    "import time\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import os\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    ''' open config file with name config_file that contains parameters\n",
    "    for this module and return Python object\n",
    "\n",
    "    Args:\n",
    "        config_file: filename containing config parameters\n",
    "\n",
    "    Returns:\n",
    "        config: Python dictionary with config parms from config file - dictionary\n",
    "\n",
    "\n",
    "    '''\n",
    "    current_path = os.getcwd()\n",
    "    path_to_yaml = os.path.join(current_path, config_file)\n",
    "    print(\"path_to_yaml \" + path_to_yaml)\n",
    "    try:\n",
    "        with open(path_to_yaml, 'r') as c_file:\n",
    "            config = yaml.safe_load(c_file)\n",
    "        return config\n",
    "    except Exception as error:\n",
    "        print('Error reading the config file ' + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_values(config):\n",
    "    for val in config:\n",
    "        print(\"config value \",val,\" \",str(config[val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    ''' get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "\n",
    "    '''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory\n",
    "    # containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path,input_csv,pickled_input_dataframe,save_raw_dataframe,load_from_scratch):\n",
    "    ''' load data into dataframe\n",
    "    Args:\n",
    "        path: path containing input file\n",
    "        input_csv: input file name\n",
    "        pickled_input_dataframe: pickled version of input file\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "    '''\n",
    "    if load_from_scratch:\n",
    "        # if loading from scratch, the raw CSV file is expected to be in the data directory which is a sibling to the \n",
    "        # directory that contains this notebook\n",
    "        unpickled_df = pd.read_csv(os.path.join(path,input_csv)) \n",
    "        if save_raw_dataframe:\n",
    "            file_name = os.path.join(path,pickled_input_dataframe)\n",
    "            print(\"file_name is \",file_name)\n",
    "            unpickled_df.to_pickle(file_name)\n",
    "    else:\n",
    "        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n",
    "        logging.debug(\"reloader done\")\n",
    "    return(unpickled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_list(x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        retur_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n",
    "    if x in list:\n",
    "        return_val = 0\n",
    "    else:\n",
    "        return_val = 1\n",
    "    return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_val(x):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n",
    "    if x >= 0:\n",
    "        return_val = 0\n",
    "    else:\n",
    "        return_val = 1\n",
    "    return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_assessment(df,columns,valid_values,non_neg_continuous):\n",
    "    ''' assess the values in a dataframe\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    '''\n",
    "    for col in list(df):\n",
    "        print(\"Missing values in \",col,\" \",str(df[col].isna().sum()))\n",
    "        print(\"Distinct values in \",col,\" \",str(df[col].nunique()))\n",
    "    # for categorical columns with a limited number of valid values, count the number of invalid values by column\n",
    "    for col in valid_values:\n",
    "        print(\"non-valid values in column \",col,\" \",str(df[col].apply(lambda x:not_in_list(x,valid_values[col])).sum()))\n",
    "    # count non-numeric values in continuous columns\n",
    "    for col in columns['continuous']:\n",
    "        # mask = pd.to_numeric(df['Hours_Worked'], errors='coerce').isna()\n",
    "        mask = pd.to_numeric(df[col], errors='coerce').isna()\n",
    "        print(\"non-numeric values in continuous col \",col,\" \",str(mask.sum()))\n",
    "        # if there are no non-numeric values in the column and it muast have non-negative values, count negative values\n",
    "        if (mask.sum()==0) and (col in non_neg_continuous):\n",
    "            print(\"negative values in colum \",col,\" \",str(df[col].apply(lambda x:neg_val(x)).sum()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_range(x,max,min):\n",
    "    ''' count whether a value is in a range\n",
    "    Args:\n",
    "        x: value to check in range\n",
    "        max: top of the range to check\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''\n",
    "    if x > max or x < min:\n",
    "        return_val = 1\n",
    "    else:\n",
    "        return_val = 0\n",
    "    return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_bounding_box(latitude,longitude,bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''    \n",
    "    if ((latitude <= bounding_box['max_lat']) and (latitude >= bounding_box['min_lat'])) \\\n",
    "    and ((longitude <= bounding_box['max_long']) and (longitude >= bounding_box['min_long'])):\n",
    "         ret_val = 0\n",
    "    else:\n",
    "         ret_val = 1\n",
    "    return(ret_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_assessment(df,bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    ''' \n",
    "    # count the number of entries in the latitude column that are above or below a given amount\n",
    "    # df['col_3'] = df.apply(lambda x: get_sublist(x.col_1, x.col_2), axis=1)\n",
    "    print(\"latitude out of bounds count \",str(df['latitude'].apply(lambda x:out_of_range(x,bounding_box['max_lat'],bounding_box['min_lat'])).sum()))\n",
    "    print(\"longitude out of bounds count \",str(df['longitude'].apply(lambda x:out_of_range(x,bounding_box['max_long'],bounding_box['min_long'])).sum()))\n",
    "    print(\"location out of bounds count \",str(df.apply(lambda x: out_of_bounding_box(x.latitude,x.longitude,bounding_box), axis=1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master cell\n",
    "This cell contains calls to the other functions in this notebook to complete the data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "path = get_path()\n",
    "print(\"path is \",path)\n",
    "config = get_config('data_preparation_config.yml')\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.warning(\"logging check\")\n",
    "print_config_values(config)\n",
    "# load dataframe and, if parameter set, save CSV file as a pickled dataframe\n",
    "df = ingest_data(path,config['file_names']['input_csv'],config['file_names']['pickled_input_dataframe'],config['general']['save_raw_dataframe'],config['general']['load_from_scratch'])\n",
    "# get basic assessment information for the dataframe\n",
    "basic_assessment(df,config['columns'],config['valid_values'],config['non_negative_continuous'])\n",
    "# get assessment for geospatial information\n",
    "geo_assessment(df,config['bounding_box'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
